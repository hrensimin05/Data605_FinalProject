---
title: "DATA 605 Final Project"
author: "Dominika Markowska-Desvallons"
date: "5/24/2021"
output: html_document
---
```{r, message=FALSE, warning=FALSE}
library(gridExtra)
library(RColorBrewer)
library(Matrix)
library(scales)
library(corrplot)
library(MASS)
library(psych)
library(ggplot2)
library(matlib)
library(dplyr)
library(tidyr)
library(kableExtra)
library(purrr)
library(Hmisc)
```
## Problem 1
Using R, generate a random variable X that has 10,000 random uniform numbers from 1 to N, where N can be any number of your choosing greater than or equal to 6.  Then generate a random variable Y that has 10,000 random normal numbers with a mean of$$\mu=\sigma=(N+1)/2$$.  
 a.   P(X>x | X>y)		b.  P(X>x, Y>y)		c.  P(X<x | X>y)				
Investigate whether P(X>x and Y>y)=P(X>x)P(Y>y) by building a table and evaluating the marginal and joint probabilities.
Check to see if independence holds by using Fisherâ€™s Exact Test and the Chi Square Test.  What is the difference between the two? Which is most appropriate?

```{r}
set.seed(666)
N <- 10
n <- 10000


X <- runif(10000, 1, N)


Y <- rnorm(10000, (N+1)/2, (N+1)/2)

```
### Probability
 Assume the small letter "x" is estimated as the median of the X variable, and the small letter "y" is estimated as the 1st quartile of the Y variable.
```{r}
x <- median(X)
x
```


```{r}
y <- quantile(Y, 0.25)
y

```

### a.  P(X>x | X>y)	

```{r}
pAll<-sum(X>x & X>y)/n 
pXy<-sum(X>y)/n 


p1=pAll/pXy

round(p1,2)
```

The probability is 0.55 or 55%. 


### b.  P(X>x, Y>y)	
```{r}
p2<-(sum(X>x & Y>y))/n
round(p2,2)

```
### c.  P(X<x | X>y)
```{r}
p3<-sum(X<x & X>y)/n
round(p3,2)
```

### Next Independence testing

Investigate whether P(X>x and Y>y) = P(X > x) * P(Y > y) by building a table and evaluating the marginal and joint probabilities.

```{r}

m<-matrix( c(sum(X>x & Y<y),sum(X>x & Y>y), sum(X<x & Y<y),sum(X<x & Y>y)), nrow = 2,ncol = 2)
m<-cbind(m,c(m[1,1]+m[1,2],m[2,1]+m[2,2]))
m<-rbind(m,c(m[1,1]+m[2,1],m[1,2]+m[2,2],m[1,3]+m[2,3]))


df<-as.data.frame(m)
names(df) <- c("X>x","X<x", "Total")
row.names(df) <- c("Y<y","Y>y", "Total")
kable(df)%>%
  kable_styling(bootstrap_options = "bordered")
pm<-m/m[3,3]
dfp<-as.data.frame(pm)
names(dfp) <- c("X>x","X<x", "Total")
row.names(dfp) <- c("Y<y","Y>y", "Total")
kable(round(dfp,2)) %>%
  kable_styling(bootstrap_options = "bordered")
```

#### Calculating

```{r}
#P(X>x)P(Y>y)
p1<-pm[3,1]*pm[2,3]
p1
```
```{r}
#P(X>x and Y>y)
p2<-round(pm[2,1],digits = 3)
p2
```
```{r}
#P(X>x and Y>y)=P(X>x)P(Y>y)
p1==p2
```
```{r}
chisq.test(m, correct=TRUE)
```

```{r}

fisher.test(m, simulate.p.value=TRUE)
```
We use the chi square test is used when the cell sizes are large, which would be appropriate to use in this case. 


## Problem Two
You are to register for Kaggle.com (free) and compete in the House Prices: Advanced Regression Techniques competition.  https://www.kaggle.com/c/house-prices-advanced-regression-techniques .  I want you to do the following.

## Descriptive and Inferential Statistics

```{r}

train <- read.csv('https://raw.githubusercontent.com/hrensimin05/Data605_FinalProject/main/train.csv')
test<-read.csv('https://raw.githubusercontent.com/hrensimin05/Data605_FinalProject/main/test.csv')

dim(train)

```

```{r}
summary(train)
```

#### Variables

```{r}
ggplot(train, aes(x = YearBuilt, y = SalePrice)) +
  geom_point()+
  geom_smooth(method=lm) +
  scale_y_continuous(labels = scales::comma)

```


```{r}
ggplot(train, aes(x = OverallQual, y = SalePrice)) +
  geom_point()+
  geom_smooth(method=lm) +
  scale_y_continuous(labels = scales::comma)+coord_flip()
``` 

```{r}
ggplot(train, aes(x = Neighborhood, y = SalePrice)) +
  geom_point()+
  geom_smooth(method=lm) +
  scale_y_continuous(labels = scales::comma)+ coord_flip()
``` 

#### Correlation

```{r}

data=select(train,YearBuilt,OverallQual,SalePrice)

mat=cor(data)


corrplot(mat,method ="color")
```

#### Hypothesis Testing
SalePrice and YearBuilt

```{r}

cor.test(train$SalePrice,train$YearBuilt, conf.level = 0.8)

```

SalePrice and OverallQual


```{r}

cor.test(train$SalePrice,train$OverallQual, conf.level = 0.8)

```

SalePrice and FullBath


```{r}

cor.test(train$SalePrice,train$FullBath, conf.level = 0.8)

```

We can see from above examples of variables that the correlation is not equal to 0 and with 80 percent confidence that there is correlation of  0.5 and 0.55, 0.78 and 0.8, and 0.54 and 0.58 respectively. 

The familywise error rate (FWE or FWER) is the probability of a coming to at least one false conclusion in a series of hypothesis tests.I would not worried about it in our case due to the p-value > 0.05 which means that the  p-value works as an alternate for the rejections point as they provide the smallest level of significance under which the null hypothesis is not true. 

## Linear Algebra and Correlation.

```{r}
pmatrix <- solve(mat)
print(pmatrix)

```

```{r}
round(mat %*% pmatrix,4)
```


```{r}
pcmat<-round(pmatrix %*% mat,4)
pcmat
```
The precision matrix is an inverse of the correlation matrix, multiplying them in either direction gives us an identity matrix.

#### LU decomposition
Correlation Matrix

```{r}
library(matrixcalc)


lu.decomposition(pcmat)
```
### Calculus-Based Probability & Statistic

```{r}
z <- train$TotalBsmtSF 

min(z)

```
```{r}
hist(z)
```
Then load the MASS package and run fitdistr to fit an exponential probability density function.


```{r}
fit <-fitdistr(z, densfun = "exponential")
fit
```
```{r}

fit$estimate
sample<-rexp(1000, fit$estimate)

```
```{r}

par(mfrow=c(1,2))
hist(z, breaks = 100, xlab = "Observed", main = "Observed")
hist(sample, breaks = 100, xlab = "Simulated", main = "Simulated")
```

We can notice from the histograms that the simulated data is more heavily skewed to the right while the observed data is more concentrated to the center.

```{r}
quantile(sample, probs = c(0.05, 0.95))

```
```{r}
#lower
mean(train$TotalBsmtSF ) - qnorm(0.95) * sd(train$TotalBsmtSF) / sqrt(length(train$TotalBsmtSF))
```


```{r}
#upper
mean(train$TotalBsmtSF ) + qnorm(0.95) * sd(train$TotalBsmtSF ) / sqrt(length(train$TotalBsmtSF ))

```
```{r}
quantile(train$TotalBsmtSF , probs=c(.05,.95))
```
For TotalBsmtSF, 95% CI is 1038 < X < 1076.

5th percentile is 519 and 95th percentile is 1753.

### Modeling
Training Data and Model Generation

```{r}

dt <- sapply(train, is.numeric)
dt_df <- train[ , dt]

head(dt_df)


```

```{r}
#Find correlation for Sale Prices

c_prices <-data.frame(apply(dt_df,2, function(col)cor(col, dt_df$SalePrice, use = "complete.obs")))
colnames(c_prices) <- c("cor")
c_prices


```

```{r}
(subset(c_prices, cor > 0.5))

```

```{r}


model <- lm(SalePrice ~ OverallQual + YearBuilt + YearRemodAdd + TotalBsmtSF + X1stFlrSF + GrLivArea + FullBath + TotRmsAbvGrd + GarageCars + GarageArea, data = train)

summary(model)

```

So  the R^2 value of 0.7737, 77.37% of the variance can be explained by this model.

```{r}

myprediction <- predict(model,test)


DMDmodel <- data.frame( Id = test[,"Id"],  SalePrice = myprediction)
DMDmodel[DMDmodel<0] <- 0
DMDmodel <- replace(DMDmodel,is.na(DMDmodel),0)
  
head(DMDmodel)
```
```{r}
write.csv(DMDmodel, file="DMDmodel.csv", row.names = FALSE)

```

The resulting multivariate model explains 77.37% of the data with statistically significat p-values for the choosen variables. The residual standard error, the standard deviation of the residuals, is 37920 on 1449 degrees of freedom, so the predicted price will deviate from the actual price by a mean of 37920.





## Kaggle Submission - Score

Report your Kaggle.com user name and score.

Kaggle Username: hrensimin05

Kaggle Score: 0.79801

See submission documents : https://github.com/hrensimin05/Data605_FinalProject


https://rpubs.com/hrensimin05/774107


















